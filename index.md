---
layout: page
title: FoMo lectures
---

# FoMo: Foundation models in Amsterdam

The FoMo lectures is a series of seminars highlighting the work being done in Amsterdam around _foundation models_: large, highly re-usable machine learning models trained on great amounts of data. These include large language models like GPT and generative vision models like DALLE·2, Stable Diffusion and Midjourney.

The talks are held alternately at the UvA and VU, and feature speakers from a variety of domains. They are intended for a technical audience.

# Upcoming talks

<table><tr>
  <th>date</th><th>time</th><th>location</th><th>talks</th>
</tr><tr>
<td valign="top">June 27</td><td valign="top"> 12:00 </td><td valign="top"> VU, room tbd </td><td markdown="1"> 
Tao Hu (UvA)
title to be announced.
  
Stefan Schouten (VU, CLTL)
title to be announced.
</td>
</tr><tr>
<td valign="top">June 20</td><td valign="top"> 12:00 </td><td valign="top"> L3.36, Lab42 at UvA </td><td markdown="1"> 
Selene Baez Santamaria (VU, CLTL)
title to be announced.  

Second speaker to be announced.
</td>
</tr><tr>
<td valign="top">June 13</td><td valign="top"> 12:00 </td><td valign="top"> NU-3A06 at VU </td><td markdown="1"> 
Gabriel Bénédict (UvA) 
**RecFusion: A Binomial Diffusion Process for 1D Data for Recommendation**
Generative Information Retrieval (a.k.a. Generative Neural Search or chatGPT + attribution + no-hallucination) has experienced substantial growth across multiple research communities and has been highly visible in the popular press. Theoretical, empirical, and actual user-facing products have been released that retrieve documents (via generation) (Generative Document Retrieval) or directly generate answers given an input request (Grounded Answer Generation).
A subfield of Generative IR, Generative Recommendations, is still in its infancy. We propose RecFusion to use diffusion models to generate recommendations. We benchmark classical diffusion formulations (normal distribution for the forward and backward diffusion process, Unets and ELBO) against formulations fitted to the RecSys setting:  1D diffusion (user-by-user), binomial diffusion and multinomial loss (like in MultVAE). We also experiment with diffusion guidance to condition the generation of recommendation strips on movie genre (a.k.a. controllable recommendation).


Wouter van Atteveldt (VU)
**Are LLMs and Transfer Learning a game changer for Computational Social Science?**
A core part of computational social science is extracting structured data such as political topic or stances from unstructured data such as news, twitter, or tiktok feeds. Although Supervised Machine Learning has been part of our toolkit for at least two decades, it has traditionally suffered from data scarcity problems as the complex and shifting nature of social science concepts are unsuitable for large standardized data sets such as common in computer vision and NLP. BERT and other pre-trained models may well be a game changer here, as they can offer valid results even with relatively small data sets. We show empirically how BERT and BERT-NLI can be used for valid measurement of political communication concepts in a number of settings.

</td>
</tr><tr>
<td valign="top">June 6</td><td valign="top"> 12:00 </td><td valign="top"> L3.36, Lab42 at UvA</td> <td markdown="1">
Jan-Christoph Kalo (VU, L&R group)
**Knowledge Graph Construction with Large Language Models**
In recent years, researchers have started exploring the capabilities of Large Language Models (LLMs) to store relational knowledge. A controversial paper, Language Models as Knowledge Bases claimed in 2019 that LLMs might be able to replace knowledge graphs.
In this talk, we investigate the question if LLMs can replace KGs. We compare various methods for extracting factual knowledge from LLMs. Furthermore, we empirically evaluate what kind of knowledge is actually learned by LLMs.

Ivona Najdenkoska (UvA, AIM lab)
**Bridging Vision and Language for Multimodal Few-shot Learning**
Language models have made significant progress in recent years, particularly in scenarios with limited labeled data. These advancements in natural language processing have inspired similar efforts in the vision domain, resulting in models that demonstrate impressive few-shot and zero-shot image classification capabilities. However, the substantial domain gap between vision and language modalities presents a non-trivial challenge for combining them as multimodal few-shot learners. Visual data, characterized by pixel values, and language, reliant on symbolic representations, pose difficulties in aligning and integrating these modalities, while keeping their few-shot abilities. Existing methods attempt to communicate visual concepts as "prompts" to frozen language models, but they often rely on hand-engineered task induction to reduce the search space, which is not always optimal. This talk will present novel approaches for multimodal few-shot learning, that leverage the few-shot capabilities of large-scale pre-trained language models, while also emphasizing computational efficiency.
</td></tr>
</table>



