---
title: FoMo lectures
---

# FoMo: Foundation models in Amsterdam

The FoMo lectures is a series of seminars highlighting the work being done in Amsterdam around _foundation models_: large, highly re-usable machine learning models trained on great amounts of data. These include large language models like GPT and generative vision models like DALLEÂ·2, Stable Diffusion and Midjourney.

The talks are held alternately at the UvA and VU, and feature speakers from a variety of domains. They are intended for a technical audience.

# Upcoming talks

---
|date|time|location|talks|
---
|June 27| 12:00 | VU, room tbd | Jan-Christoph Kalo (VU, L&R group)
Knowledge Graph Construction with Large Language Models
In recent years, researchers have started exploring the capabilities of Large Language Models (LLMs) to store relational knowledge. A controversial paper, Language Models as Knowledge Bases claimed in 2019 that LLMs might be able to replace knowledge graphs.
In this talk, we investigate the question if LLMs can replace KGs. We compare various methods for extracting factual knowledge from LLMs. Furthermore, we empirically evaluate what kind of knowledge is actually learned by LLMs.

Ivona Najdenkoska (UvA, AIM lab)
Bridging Vision and Language for Multimodal Few-shot Learning
Language models have made significant progress in recent years, particularly in scenarios with limited labeled data. These advancements in natural language processing have inspired similar efforts in the vision domain, resulting in models that demonstrate impressive few-shot and zero-shot image classification capabilities. However, the substantial domain gap between vision and language modalities presents a non-trivial challenge for combining them as multimodal few-shot learners. Visual data, characterized by pixel values, and language, reliant on symbolic representations, pose difficulties in aligning and integrating these modalities, while keeping their few-shot abilities. Existing methods attempt to communicate visual concepts as "prompts" to frozen language models, but they often rely on hand-engineered task induction to reduce the search space, which is not always optimal. This talk will present novel approaches for multimodal few-shot learning, that leverage the few-shot capabilities of large-scale pre-trained language models, while also emphasizing computational efficiency.
|
|June 20| 12:00 | L3.36, Lab42 at UvA | |
|June 13| 12:00 | NU-3A06 at VU | |
|June 6 | 12:00 | L3.36, Lab42 at UvA ||



